{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Classification Problem End-to-End using Scikit-Learn","metadata":{}},{"cell_type":"code","source":"## Step 1: Import Necessary Libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification","metadata":{},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"## Step 2: Create a Synthetic Dataset\n\n# Create a synthetic dataset\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=5, n_classes=2, random_state=42)\n\n# Convert to DataFrame\ndf = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(20)])\ndf['target'] = y","metadata":{},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"## Step 3: Data Cleaning\n\n# Check for missing values\nprint(df.isnull().sum())\n\n# No missing values in the synthetic dataset","metadata":{},"execution_count":3,"outputs":[{"name":"stdout","output_type":"stream","text":"feature_0     0\n\nfeature_1     0\n\nfeature_2     0\n\nfeature_3     0\n\nfeature_4     0\n\nfeature_5     0\n\nfeature_6     0\n\nfeature_7     0\n\nfeature_8     0\n\nfeature_9     0\n\nfeature_10    0\n\nfeature_11    0\n\nfeature_12    0\n\nfeature_13    0\n\nfeature_14    0\n\nfeature_15    0\n\nfeature_16    0\n\nfeature_17    0\n\nfeature_18    0\n\nfeature_19    0\n\ntarget        0\n\ndtype: int64\n"}]},{"cell_type":"code","source":"## Step 4: Exploratory Data Analysis (EDA)\n\n# Basic statistics\nprint(df.describe())","metadata":{},"execution_count":4,"outputs":[{"name":"stdout","output_type":"stream","text":"         feature_0   feature_1   feature_2   feature_3   feature_4  feature_5  \\\n\ncount  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000     1000.0   \n\nmean      0.007694    -0.028220    -0.025372    -0.022693     -0.031288        0.0   \n\nstd        0.977071     0.973869     1.018242     0.988278      1.003315        1.0   \n\nmin       -3.506630    -3.238711    -3.471189    -3.352754     -3.542096       -1.0   \n\n25%       -0.606487    -0.686074    -0.705453    -0.701672     -0.705882       -1.0   \n\n50%        0.004755    -0.013539    -0.041241    -0.010865     -0.027982        0.0   \n\n75%        0.671467     0.615209     0.648697     0.634989      0.643979        1.0   \n\nmax        3.210818     3.102296     3.842128     3.291114      3.795230        1.0   \n\n\n\n        feature_6   feature_7   feature_8   feature_9  ...  feature_12  \\\n\ncount  1000.000000  1000.000000  1000.000000  1000.000000  ...  1000.000000   \n\nmean      0.000292     0.026907     0.005692     0.040354  ...    -0.003206   \n\nstd        0.989693     1.015268     1.012827     1.008010  ...     0.987191   \n\nmin       -3.400353    -3.227258    -3.509664    -3.362067  ...    -3.119850   \n\n25%       -0.676578    -0.681171    -0.688570    -0.637885  ...    -0.669380   \n\n50%        0.016108     0.038333    -0.010745     0.022603  ...    -0.041545   \n\n75%        0.708268     0.743175     0.688824     0.712640  ...     0.673015   \n\nmax        3.176336     3.356266     3.225352     3.212196  ...     3.193213   \n\n\n\n        feature_13  feature_14  feature_15  feature_16  feature_17  feature_18  \\\n\ncount  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n\nmean      0.028531     0.033343     0.034286    -0.007700     0.008933     0.010610   \n\nstd        0.986633     0.973623     1.023083     1.001927     0.985418     0.996656   \n\nmin       -3.064084    -2.993207    -3.562436    -3.563276    -3.323479    -3.551530   \n\n25%       -0.621846    -0.681484    -0.645809    -0.661548    -0.662926    -0.641721   \n\n50%        0.041520     0.050574     0.065039     0.014967     0.002156     0.004625   \n\n75%        0.726590     0.671774     0.723778     0.627994     0.689302     0.673204   \n\nmax        3.086745     3.329141     3.560122     3.648248     3.268637     3.456037   \n\n\n\n        feature_19       target  \n\ncount  1000.000000  1000.000000  \n\nmean     -0.009423     0.508000  \n\nstd       0.979861     0.500134  \n\nmin      -3.089511     0.000000  \n\n25%      -0.697421     0.000000  \n\n50%       0.002850     1.000000  \n\n75%       0.673862     1.000000  \n\nmax       3.094420     1.000000  \n"}]},{"cell_type":"code","source":"# Pairplot for a subset of features\nsns.pairplot(df[['feature_0', 'feature_1', 'feature_2', 'feature_3', 'target']], hue='target')\nplt.show()","metadata":{},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Correlation matrix\nplt.figure(figsize=(12, 8))\nsns.heatmap(df.corr(), annot=True, cmap='coolwarm')\nplt.show()","metadata":{},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"## Step 5: Data Preprocessing\n\n# Split the data\nX = df.drop('target', axis=1)\ny = df['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Scale the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","metadata":{},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"## Step 6: Model Building\n\n# Initialize the model\nrfc = RandomForestClassifier(random_state=42)\n\n# Fit the model\nrfc.fit(X_train_scaled, y_train)","metadata":{},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"## Step 7: Model Prediction\n\n# Predictions\ny_pred = rfc.predict(X_test_scaled)","metadata":{},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"## Step 8: Model Evaluation\n\n# Confusion Matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(conf_matrix)","metadata":{},"execution_count":10,"outputs":[{"name":"stdout","output_type":"stream","text":"[[86 10]\n\n [16 88]]\n"}]},{"cell_type":"code","source":"# Classification Report\nclass_report = classification_report(y_test, y_pred)\nprint(class_report)","metadata":{},"execution_count":11,"outputs":[{"name":"stdout","output_type":"stream","text":"              precision    recall  f1-score   support\n\n\n\n           0       0.84      0.90      0.87        96\n\n           1       0.90      0.85      0.87       104\n\n\n\n    accuracy                           0.87       200\n\n   macro avg       0.87      0.87      0.87       200\n\nweighted avg       0.87      0.87      0.87       200\n"}]},{"cell_type":"code","source":"# Accuracy Score\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Accuracy: {accuracy}')","metadata":{},"execution_count":12,"outputs":[{"name":"stdout","output_type":"stream","text":"Accuracy: 0.87\n"}]},{"cell_type":"code","source":"## Step 9: Hyperparameter Tuning\n\n# Define the parameter grid\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\n# Initialize GridSearchCV\ngrid_search = GridSearchCV(estimator=rfc, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n\n# Fit GridSearchCV\ngrid_search.fit(X_train_scaled, y_train)\n\n# Best parameters and best score\nprint(f'Best Parameters: {grid_search.best_params_}')\nprint(f'Best Score: {grid_search.best_score_}')","metadata":{},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":14,"outputs":[{"name":"stdout","output_type":"stream","text":"Best Parameters: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n\nBest Score: 0.8987499999999999\n"}]},{"cell_type":"code","source":"## Step 10: Final Model Evaluation\n\n# Best model predictions\nbest_model = grid_search.best_estimator_\ny_pred_best = best_model.predict(X_test_scaled)\n\n# Confusion Matrix\nconf_matrix_best = confusion_matrix(y_test, y_pred_best)\nprint(conf_matrix_best)","metadata":{},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":16,"outputs":[{"name":"stdout","output_type":"stream","text":"[[86 10]\n\n [15 89]]\n"}]},{"cell_type":"code","source":"# Classification Report\nclass_report_best = classification_report(y_test, y_pred_best)\nprint(class_report_best)","metadata":{},"execution_count":17,"outputs":[{"name":"stdout","output_type":"stream","text":"              precision    recall  f1-score   support\n\n\n\n           0       0.85      0.90      0.87        96\n\n           1       0.90      0.86      0.88       104\n\n\n\n    accuracy                           0.87       200\n\n   macro avg       0.87      0.87      0.87       200\n\nweighted avg       0.87      0.87      0.87       200\n"}]},{"cell_type":"code","source":"# Accuracy Score\naccuracy_best = accuracy_score(y_test, y_pred_best)\nprint(f'Accuracy: {accuracy_best}')","metadata":{},"execution_count":18,"outputs":[{"name":"stdout","output_type":"stream","text":"Accuracy: 0.875\n"}]}]}